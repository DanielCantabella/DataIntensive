{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767f851d",
   "metadata": {},
   "source": [
    "## 1. Hack Preprocessing\n",
    "\n",
    "Goal is to take an array of our downloaded raw files, extracting the description fields and saving them in corresponding single column csv file with the classifier as the header\n",
    "\n",
    "This notebook assumes that the \"rawData\" directory on hdfs holds the data from our sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ddf803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\tfilename\ttitle\tcontent\n",
      "business\t001.txt\tAd sales boost Time Warner profit\t Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.  The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.  Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.  Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.  TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(category\tfilename\ttitle\tcontent, \"business\t001.txt\tAd sales boost Time Warner profit\t Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.  The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quart..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//The root to where the rawData, processedData and dataSets directories are\n",
    "val root = \"hdfs://localhost:9000/project/\";\n",
    "\n",
    "//An array holding all the file names in rawData \n",
    "//(We reuse the same names in the processed data)\n",
    "val fileNames = Array(\"bbc-news-data.csv\",\"goodreads_data.csv\",\"job_postings.csv\",\"mtsamples.csv\");\n",
    "\n",
    "//Test that the raw data can be accessed\n",
    "val file = root + \"rawData/bbc-news-data.csv\";\n",
    "val rawRDD = sc.textFile(file);\n",
    "val rawRDDtop = rawRDD.take(2);\n",
    "rawRDDtop.foreach(println);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a623c9c",
   "metadata": {},
   "source": [
    "#### Creating dataFrames for each of the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdeb088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@5a365f8e\n",
       "newsData = [category: string, filename: string ... 2 more fields]\n",
       "bookData = [_c0: string, Book: string ... 6 more fields]\n",
       "jobData = [job_id: string, company_id: string ... 25 more fields]\n",
       "medicalData = [_c0: int, description: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: int, description: string ... 4 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"ID2221 Project\").master(\"local[*]\").getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "//Reads the first file (news data)\n",
    "val newsData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\",\"\\t\").option(\"multiLine\", \"true\")\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\").option(\"ignoreTrailingWhiteSpace\", \"true\").csv(root + \"rawData/\" + fileNames(0));\n",
    "//newsData.show(5);\n",
    "//newsData.printSchema();\n",
    "\n",
    "//Reads the second file (book data)\n",
    "val bookData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\").option(\"multiLine\", \"true\").csv(root + \"rawData/\" + fileNames(1));\n",
    "//bookData.show(5);\n",
    "//bookData.printSchema();\n",
    "\n",
    "//Reads the third file (job postings)  (It does not seem to parse correctly but it is sufficient to show and test my function)\n",
    "val jobData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "                 .option(\"ignoreTrailingWhiteSpace\", \"true\").option(\"multiLine\", \"true\").csv(root + \"rawData/\" + fileNames(2));\n",
    "//jobData.show(5);\n",
    "//jobData.printSchema();\n",
    "\n",
    "//Reads the fourth file (medical transcripts)\n",
    "val medicalData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "                 .option(\"ignoreTrailingWhiteSpace\", \"true\").option(\"multiLine\", \"true\").csv(root + \"rawData/\" + fileNames(3));\n",
    "//medicalData.show(5);\n",
    "//medicalData.printSchema();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c6fd7",
   "metadata": {},
   "source": [
    "#### Extracting and pre-processing each of the dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f67e9a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in thread \"Executor task launch worker for task 0.0 in stage 65.0 (TID 188)\" java.lang.SecurityException: Not allowed to invoke System.exit!\n",
      "\tat org.apache.toree.security.KernelSecurityManager.checkExit(KernelSecurityManager.scala:133)\n",
      "\tat java.lang.Runtime.halt(Runtime.java:264)\n",
      "\tat org.apache.spark.util.SparkUncaughtExceptionHandler.uncaughtException(SparkUncaughtExceptionHandler.scala:70)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:772)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": "Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 188) (172.27.163.63 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\nDriver stacktrace:",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 65.0 failed 1 times, most recent failure: Lost task 0.0 in stage 65.0 (TID 188) (172.27.163.63 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)",
      "  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:1020)",
      "  at $anonfun$new$1(<console>:76)",
      "  at $anonfun$new$1$adapted(<console>:70)",
      "  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)",
      "  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)",
      "  ... 43 elided",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "//Creates dataframe with natural language under column \"news article\"\n",
    "val newsNL = newsData.select(newsData(\"content\").alias(\"news article\"));\n",
    "\n",
    "//Creates dataframe with natural language under column \"book description\"\n",
    "val bookNL = bookData.select(bookData(\"Description\").alias(\"book description\"));\n",
    "\n",
    "//Creates dataframe with natural language under column \"job posting\"\n",
    "val jobNL = jobData.select(jobData(\"description\").alias(\"job posting\"));\n",
    "\n",
    "//Creates dataframe with natural language under column \"medical transcript\"\n",
    "val medicalNL = medicalData.select(medicalData(\"transcription\").alias(\"medical transcript\"));\n",
    "\n",
    "val dfNLArray = Array(newsNL, bookNL, jobNL, medicalNL);\n",
    "\n",
    "\n",
    "//Pre-processing to remove punctuation and whitespace, make text lower case and cut the text which is very long.\n",
    "\n",
    "//Removes punctuations, citations and whitespace and makes text lower case\n",
    "import scala.util.matching \n",
    "var index = 0;\n",
    "val roughProcessedDF : Array[DataFrame] = new Array[DataFrame](4);\n",
    "for (df <- dfNLArray) {\n",
    "    //First makes lowercase\n",
    "    //Then replaces all whitespace, - and _ with a single blank space\n",
    "    //Finally removes all punctations and trailing/leading spaces with an empty string\n",
    "    val tempRDD = df.rdd.map(x => (x.toString.toLowerCase.replaceAll(\"\"\"\\-|_|\\s+\"\"\",\" \")\n",
    "                             .replaceAll(\"\"\"\\p{Punct}|^\\s+|\\s+$\"\"\", \"\"),0)); //Makes a temporary tupple to allow for easier data frame creation\n",
    "    roughProcessedDF(index) = spark.createDataFrame(tempRDD.collect().toSeq).toDF(df.columns(0),\"garbage\").select(df.columns(0));\n",
    "    index = index + 1;\n",
    "}\n",
    "\n",
    "//Finds the average length of documents\n",
    "var totalLength : Long = 0;\n",
    "var totalCount : Long = 0;\n",
    "for (df <- roughProcessedDF) {\n",
    "    totalCount = totalCount + df.count();\n",
    "    var length : Long = 0;\n",
    "    for (row <- df.collect())\n",
    "        length = length + row.mkString.length;\n",
    "    totalLength = totalLength + length;\n",
    "}\n",
    "val averageLength = totalLength / totalCount;\n",
    "println(s\"The average length of the documents is $averageLength characters\")\n",
    "\n",
    "///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "// This shit gives an oom error. I cannot for the life of me avoid the collect call which causes it. Kill me now //\n",
    "///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "//Cut the documents which are longer than average to make all the documents have similar length\n",
    "val processedDF : Array[DataFrame] = new Array[DataFrame](4);\n",
    "index = 0;\n",
    "for (df <- roughProcessedDF) {\n",
    "    val tempRDD = df.rdd.map(x => (x.toString.slice(0,averageLength.asInstanceOf[Int]),0)); //Makes a temporary tupple to allow for easier data frame creation\n",
    "    //processedDF(index) = spark.createDataFrame(tempRDD.collect().toSeq).toDF(df.columns(0),\"garbage\").select(df.columns(0));\n",
    "    processedDF(index) = tempRDD.toDF(df.columns(0),\"garbage\").select(df.columns(0));\n",
    "    index = index + 1;\n",
    "}\n",
    "\n",
    "for (df <- processedDF) {\n",
    "    df.show(10);\n",
    "    println(df.count());\n",
    "}\n",
    "\n",
    "\n",
    "/*\n",
    "val tempRDD = medicalNL.rdd.take(2).map(x => (x.toString.toLowerCase.replaceAll(\"\"\"\\-|_|\\s+\"\"\",\" \")\n",
    "                             .replaceAll(\"\"\"\\p{Punct}|^\\s+|\\s+$\"\"\", \"\"),0))\n",
    "println(tempRDD(0))\n",
    "val tempSeq = tempRDD.toSeq\n",
    "val tempDF = spark.createDataFrame(tempSeq).toDF(\"medical\",\"garbage\").select(\"medical\");\n",
    "*/\n",
    "\n",
    "//tempDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b7eaebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "temps = test\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "test"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val temps : String = \"test\"\n",
    "println(temps.slice(0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc68b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:37: error: not found: value roughProcessedDF\n       roughProcessedDF(0).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedData/\" + fileNames(0));\n       ^\n<console>:38: error: not found: value roughProcessedDF\n       roughProcessedDF(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedData/\" + fileNames(1));\n       ^\n<console>:39: error: not found: value roughProcessedDF\n       roughProcessedDF(2).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedData/\" + fileNames(2));\n       ^\n<console>:40: error: not found: value roughProcessedDF\n       roughProcessedDF(3).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedData/\" + fileNames(3));\n       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode.Overwrite\n",
    "//Saves the dataframes as processed csv files\n",
    "//It just ignores this and then reads something. Not sure what since the order is off from the regular data\n",
    "//The news articles seems to read from entry 2004 dafaq\n",
    "//Might have something to do with the console outputting \"Notebook createDataSetsFunction.ipynb is not trusted\"\n",
    "//It can create new files but not remove old files?\n",
    "roughProcessedDF(0).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedData/\" + fileNames(0));\n",
    "roughProcessedDF(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedData/\" + fileNames(1));\n",
    "roughProcessedDF(2).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedData/\" + fileNames(2));\n",
    "roughProcessedDF(3).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedData/\" + fileNames(3));\n",
    "\n",
    "//Checks that data saved correctly\n",
    "for (file <- fileNames) {\n",
    "    val temp = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"processedData/\" + file);\n",
    "    temp.show(5);\n",
    "    println(temp.count());\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343ad14",
   "metadata": {},
   "source": [
    "## 2. Main Function TODO\n",
    "\n",
    "Goal: Should produce three balanced sets of data. The sets will be randomly shuffled and saved as a csv files with the headers \"classification\" and \"value\"\n",
    "\n",
    "The function should take the parameters: a list of preprocessed csv files were each file has the classifier as header and the values as entries, an array of floats detailing the relative distribution of train/test/val sets, an output directory and a desired size per class (maximum possible if none specified).\n",
    "\n",
    "The function is made in a way that there is no need for explicitly splitting the data three ways (train/val/test) we could also split it into two sets or more if we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3040d17b",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.OutOfMemoryError",
     "evalue": "GC overhead limit exceeded",
     "output_type": "error",
     "traceback": [
      "java.lang.OutOfMemoryError: GC overhead limit exceeded"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.util.Random\n",
    "\n",
    "//Messy codde for testing a bunch of things\n",
    "\n",
    "/*\n",
    "var testArray: Array[String] = Array(\"1\",\"2\",\"3\")\n",
    "testArray(2)=\"gretat\"\n",
    "testArray :+= \"that\"\n",
    "\n",
    "Random.shuffle(testArray)\n",
    "\n",
    "for (v <- testArray) {\n",
    "    println(v)\n",
    "}\n",
    "\n",
    "val testInt : Int = testArray.length\n",
    "\n",
    "val testArray2 : Array[DataFrame] = new Array[DataFrame](testInt) \n",
    "\n",
    "print(root)\n",
    "\n",
    "testArray2(2) = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"processedData/\" + \"bbc-news-data.csv\")\n",
    "\n",
    "for (v <- testArray2) {\n",
    "    println(v)\n",
    "}\n",
    "\n",
    "val testInt2 = 500;\n",
    "var testFloat = .7\n",
    "testFloat = testFloat + 0.1\n",
    "\n",
    "println((testInt2 * testFloat).asInstanceOf[Int])\n",
    "\n",
    "var testArray3: Array[Float] = Array(1,2,3)\n",
    "\n",
    "for(v <- 0 to testArray3.length-1)\n",
    "    testArray3(v)=testArray3(v)/3\n",
    "for (v <- testArray3) {\n",
    "    println(v)\n",
    "}\n",
    "\n",
    "scala.util.Random.alphanumeric.take(20)\n",
    "*/\n",
    "\n",
    "val df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"processedData/\" + \"bbc-news-data.csv\")\n",
    "df.show(5)\n",
    "for (v <- df.take(5)) println(v)\n",
    "\n",
    "import org.apache.spark.sql.functions.rand\n",
    "\n",
    "//Shuffling \n",
    "val shuffledDF = df.orderBy(rand())\n",
    "shuffledDF.show(5)\n",
    "\n",
    "val tempDF = shuffledDF.limit(5)\n",
    "tempDF.show()\n",
    "\n",
    "for (v <- shuffledDF.take(5)) println(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa06169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n",
       "spark = org.apache.spark.sql.SparkSession@34e2edb4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "createDataSets: (processedFiles: Array[String], relativeDist: Array[Double], outputDirectory: String, outputNames: Array[String], classSize: Int)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@34e2edb4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"ID2221 Project\").master(\"local[*]\").getOrCreate();\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "//main function\n",
    "def createDataSets(processedFiles : Array[String], relativeDist : Array[Double], outputDirectory : String, \n",
    "                   outputNames : Array[String], classSize : Int = 0 ) {\n",
    "    //Creates an array holding dataframes of each input file\n",
    "    val fileDF: Array[DataFrame] = new Array[DataFrame](processedFiles.length);\n",
    "    for (i <- 0 to fileDF.length-1)\n",
    "        fileDF(i) = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(processedFiles(i));\n",
    "    \n",
    "    //Calculates desired size\n",
    "    var size : Int = classSize;\n",
    "    if (size <= 0) { //If user has not specified a desired size, the fuction will create a data set which is as large as possible\n",
    "        var minSize = fileDF(0).count();\n",
    "        for (i <- 1 to fileDF.length-1) {\n",
    "            val tempSize = fileDF(i).count();\n",
    "            if (tempSize < minSize) \n",
    "                minSize=tempSize;\n",
    "        }\n",
    "        size=minSize.asInstanceOf[Int];\n",
    "    }\n",
    "    \n",
    "    //Take \"size\" random rows from each dataframe\n",
    "    val sampledDF: Array[DataFrame] = new Array[DataFrame](fileDF.length);\n",
    "    for (i <- 0 to sampledDF.length-1) //Takes \"size\" random samples from each file\n",
    "        sampledDF(i) = fileDF(i).orderBy(rand()).limit(size);\n",
    "    \n",
    "    //Create dataframes with columns \"class\" and \"value\"\n",
    "    val pairDF: Array[DataFrame] = new Array[DataFrame](sampledDF.length);\n",
    "    for (i <- 0 to pairDF.length-1) {\n",
    "        pairDF(i)=sampledDF(i).withColumn(\"class\", lit(sampledDF(i).columns(0))); //Adds \"class\" column with value of original header\n",
    "        pairDF(i)=pairDF(i).withColumnRenamed(sampledDF(i).columns(0), \"value\"); //Renames original header to \"value\"\n",
    "    }\n",
    "    \n",
    "    //Splits the created pairs into parts according to \"relativeDist\"\n",
    "    val splitDF: Array[Array[DataFrame]] = new Array[Array[DataFrame]](sampledDF.length);\n",
    "    for (i <- 0 to splitDF.length-1) {\n",
    "        //The function sorts the results based on first column after picking random elements (we reshuffle when combining)\n",
    "        splitDF(i) = pairDF(i).randomSplit(relativeDist, 0); \n",
    "    }\n",
    "    \n",
    "    //Combines the split pairs into the desired sets (one set for each entry in \"relativeDist\")\n",
    "    val setDF: Array[DataFrame] = new Array[DataFrame](relativeDist.length);\n",
    "    for (i <- 0 to relativeDist.length-1) {\n",
    "        //Combines all the class dataframes\n",
    "        setDF(i) = splitDF(0)(i);\n",
    "        for (ii <- 1 to splitDF.length-1)\n",
    "            setDF(i) = setDF(i).union(splitDF(ii)(i));\n",
    "        \n",
    "        //Shuffles the result\n",
    "        setDF(i) = setDF(i).orderBy(rand());\n",
    "    }\n",
    "    \n",
    "    //Temporary function for showing some samples\n",
    "    for (v <- setDF) {\n",
    "        //for (e <- v) println(e.count())\n",
    "        //println(\"next\")\n",
    "        v.show(10);\n",
    "        println(v.count());\n",
    "    }\n",
    "    \n",
    "    //Saves the created sets as csv files\n",
    "    for (i <- 0 to setDF.length-1)\n",
    "        setDF(i).write.mode(\"overwrite\").option(\"header\",\"true\").csv(outputDirectory + outputNames(i));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07823d14",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.OutOfMemoryError",
     "evalue": "GC overhead limit exceeded",
     "output_type": "error",
     "traceback": [
      "java.lang.OutOfMemoryError: GC overhead limit exceeded"
     ]
    }
   ],
   "source": [
    "val rootProcessed : String = \"hdfs://localhost:9000/project/processedData/\";\n",
    "val processedFiles : Array[String] = Array(rootProcessed + \"bbc-news-data.csv\", rootProcessed + \"goodreads_data.csv\", \n",
    "                                           rootProcessed + \"job_postings.csv\", rootProcessed + \"mtsamples.csv\");\n",
    "// Just for the sake of testing 80% train, 15% val, 5% test\n",
    "val dist : Array[Double] = Array(160, 30, 10); //Any values work, function only look at comparative sizes\n",
    "\n",
    "val outputNames : Array[String] = Array(\"train\", \"val\", \"test\");\n",
    "\n",
    "createDataSets(processedFiles, dist, \"hdfs://localhost:9000/project/dataSets/\", outputNames);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0042e",
   "metadata": {},
   "source": [
    "## 3. Testing Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13d254",
   "metadata": {},
   "source": [
    "# Just trying the connection with hadoop server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs._\n",
    "\n",
    "val hadoopConf = new Configuration()\n",
    "hadoopConf.set(\"fs.defaultFS\", \"hdfs://35.175.92.154:9000\")\n",
    "hadoopConf.set(\"dfs.replication\", \"1\") // Set replication factor if needed\n",
    "// Set any other HDFS configuration parameters as needed\n",
    "\n",
    "val hdfs = FileSystem.get(hadoopConf)\n",
    "\n",
    "val path = new Path(\"/rawFiles\")\n",
    "val status = hdfs.listStatus(path)\n",
    "for (fileStatus <- status) {\n",
    "  println(fileStatus.getPath)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val filePath = \"hdfs://35.175.92.154:9000/rawFiles/goodreads_data.csv\";\n",
    "\n",
    "val df = spark.read.csv(filePath);\n",
    "df.show(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val temp = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"hdfs://35.175.92.154:9000/rawFiles/goodreads_data.csv\");\n",
    "    temp.show(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ed591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val tableName = \"tableProva\"\n",
    "val hbaseNamespace = \"your_hbase_namespace\"  // if applicable\n",
    "\n",
    "val hbaseOptions = Map(\n",
    "  \"hbase.table\" -> tableName,\n",
    "  \"hbase.namespace\" -> hbaseNamespace\n",
    ")\n",
    "\n",
    "val hbaseDF = spark.read\n",
    "  .format(\"org.apache.hadoop.hbase.spark\")\n",
    "  .options(hbaseOptions)\n",
    "  .load()\n",
    "\n",
    "hbaseDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e004b15",
   "metadata": {},
   "source": [
    "## 4. Machine Learning on created data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce05dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{IDF,HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "//Temporarily use val instead for faster testing\n",
    "val train = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"dataSets/train\");\n",
    "val test = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"dataSets/test\");\n",
    "\n",
    "//Adding column for classes as integers\n",
    "val mapper = spark.createDataFrame(Seq((\"book description\", 1.0),(\"news article\", 2.0),(\"medical transcript\", 3.0)\n",
    "                                       ,(\"job posting\", 4.0))).toDF(\"class\", \"label\");\n",
    "val trainSet = train.join(mapper, \"class\");\n",
    "val testSet = test.join(mapper, \"class\");\n",
    "\n",
    "//Number of words and documents for tf_idf\n",
    "val totalSet = train.union(test);\n",
    "val nDocs = totalSet.count();\n",
    "val nWords = totalSet.select(\"value\").flatMap(_.toString.split(\" \")).distinct().count();\n",
    "\n",
    "//Creating the pipeline for a simple classifier\n",
    "val tokenizer = new Tokenizer().setInputCol(\"value\").setOutputCol(\"words\");\n",
    "val hashTF = new HashingTF().setInputCol(tokenizer.getOutputCol).setOutputCol(\"hashTF\").setNumFeatures(nWords.asInstanceOf[Int]);\n",
    "val tf_idf = new IDF().setInputCol(hashTF.getOutputCol).setOutputCol(\"tf_idf\").setMinDocFreq(5);//nDocs.asInstanceOf[Int]);\n",
    "val model = new LogisticRegression().setMaxIter(20).setRegParam(0.01).setLabelCol(\"label\").setFeaturesCol(tf_idf.getOutputCol)\n",
    "    .setFamily(\"multinomial\");\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashTF, tf_idf, model));\n",
    "\n",
    "//Sets upp hyper parameter tuning using cross-validation\n",
    "val evaluator = new MulticlassClassificationEvaluator().setPredictionCol(\"prediction\")\n",
    "val paramGrid = new ParamGridBuilder()//.addGrid(hashingTF.numFeatures, Array(10, 100, 1000))\n",
    "    .addGrid(model.regParam, Array(0.03, 0.01, 0.1)).addGrid(model.maxIter, Array(3,10,30)).build();\n",
    "val crossValidator = new CrossValidator().setEstimator(pipeline).setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid).setNumFolds(4).setParallelism(2);\n",
    "    \n",
    "//Train the model\n",
    "val trainedModel = pipeline.fit(trainSet);     //Set parameters (Fast with decent performance)\n",
    "//val trainedModel = crossValidator.fit(trainSet); //Simple hyper parameter tuning using cross-validation (Slow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46fd7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val accuracy = evaluator.evaluate(trainedModel.transform(testSet));\n",
    "println(s\"The trained model had an accuracy of $accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f71ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Save the trained model on disk\n",
    "trainedModel.write.overwrite().save(root + \"model/trainedLogisticRegressionModel\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
