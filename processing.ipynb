{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0786e41c",
   "metadata": {},
   "source": [
    "#### Creating dataFrames for each of the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "444bdf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@4ea6e698\n",
       "root = /Users/dani/Desktop/DataIntensive/\n",
       "fileNames = Array(bbc-news-data.csv, goodreads_data.csv, job_postings.csv, mtsamples.csv)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(bbc-news-data.csv, goodreads_data.csv, job_postings.csv, mtsamples.csv)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions.{col, expr}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.util.matching \n",
    "\n",
    "val spark = SparkSession.builder().appName(\"ID2221 Project\").master(\"local[*]\").getOrCreate();\n",
    "val root = \"/Users/dani/Desktop/DataIntensive/\";\n",
    "val fileNames = Array(\"bbc-news-data.csv\",\"goodreads_data.csv\",\"job_postings.csv\",\"mtsamples.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acd31b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newsSchema = StructType(StructField(category,StringType,false),StructField(filename,StringType,false),StructField(title,StringType,false),StructField(content,StringType,false))\n",
       "bookSchema = StructType(StructField(null,StringType,true),StructField(Book,StringType,true),StructField(Author,StringType,true),StructField(Description,StringType,true),StructField(Genres,StringType,true),StructField(Avg_Rating,DoubleType,true),StructField(Num_Ratings,IntegerType,true),StructField(URL,StringType,true))\n",
       "jobSchema = StructType(StructField(job_id,IntegerType,true),StructField(company_id,IntegerType,true),StructField(title,StringType,true),StructField(description,StringType,true),Struc...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(job_id,IntegerType,true),StructField(company_id,IntegerType,true),StructField(title,StringType,true),StructField(description,StringType,true),Struc..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Schemas (not necessary but just to show what we are dealing with)\n",
    "val newsSchema = StructType(Array(\n",
    "  StructField(\"category\", StringType, nullable = false),\n",
    "  StructField(\"filename\", StringType, nullable = false),\n",
    "  StructField(\"title\", StringType, nullable = false),\n",
    "  StructField(\"content\", StringType, nullable = false)\n",
    "));\n",
    "val bookSchema = StructType(Array(\n",
    "  StructField(\"null\", StringType, nullable = true),\n",
    "  StructField(\"Book\", StringType, nullable = true),\n",
    "  StructField(\"Author\", StringType, nullable = true),\n",
    "  StructField(\"Description\", StringType, nullable = true),\n",
    "  StructField(\"Genres\", StringType, nullable = true),\n",
    "  StructField(\"Avg_Rating\", DoubleType, nullable = true),\n",
    "  StructField(\"Num_Ratings\", IntegerType, nullable = true),\n",
    "  StructField(\"URL\", StringType, nullable = true),\n",
    "));\n",
    "val jobSchema = StructType(Array(\n",
    "  StructField(\"job_id\", IntegerType, nullable = true),\n",
    "  StructField(\"company_id\", IntegerType, nullable = true),\n",
    "  StructField(\"title\", StringType, nullable = true),\n",
    "  StructField(\"description\", StringType, nullable = true),\n",
    "  StructField(\"max_salary\", IntegerType, nullable = true),\n",
    "  StructField(\"med_salary\", IntegerType, nullable = true),\n",
    "  StructField(\"min_salary\", IntegerType, nullable = true),\n",
    "  StructField(\"pay_period\", StringType, nullable = true),\n",
    "  StructField(\"formatted_work_type\", StringType, nullable = true),\n",
    "  StructField(\"location\", StringType, nullable = true),\n",
    "  StructField(\"applies\", IntegerType, nullable = true),\n",
    "  StructField(\"original_listed_time\", DoubleType, nullable = true),\n",
    "  StructField(\"remote_allowed\", IntegerType, nullable = true),\n",
    "  StructField(\"views\", IntegerType, nullable = true),\n",
    "  StructField(\"job_posting_url\", StringType, nullable = true),\n",
    "  StructField(\"application_url\", StringType, nullable = true),\n",
    "  StructField(\"application_type\", StringType, nullable = true),\n",
    "  StructField(\"expiry\", DoubleType, nullable = true),\n",
    "  StructField(\"closed_time\", DoubleType, nullable = true),\n",
    "  StructField(\"formatted_experience_level\", StringType, nullable = true),\n",
    "  StructField(\"skills_desc\", StringType, nullable = true),\n",
    "  StructField(\"listed_time\", DoubleType, nullable = true),\n",
    "  StructField(\"posting_domain\", StringType, nullable = true),\n",
    "  StructField(\"sponsored\", IntegerType, nullable = true),\n",
    "  StructField(\"work_type\", StringType, nullable = true),\n",
    "  StructField(\"currency\", StringType, nullable = true),\n",
    "  StructField(\"compensation_type\", StringType, nullable = true)\n",
    "));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f14955a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+--------------------+\n",
      "|category|filename|               title|             content|\n",
      "+--------+--------+--------------------+--------------------+\n",
      "|business| 001.txt|Ad sales boost Ti...|Quarterly profits...|\n",
      "|business| 002.txt|Dollar gains on G...|The dollar has hi...|\n",
      "|business| 003.txt|Yukos unit buyer ...|The owners of emb...|\n",
      "|business| 004.txt|High fuel prices ...|British Airways h...|\n",
      "|business| 005.txt|Pernod takeover t...|Shares in UK drin...|\n",
      "+--------+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+-------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|_c0|                Book|       Author|         Description|              Genres|          Avg_Rating|      Num_Ratings|                 URL|\n",
      "+---+--------------------+-------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|  0|To Kill a Mocking...|   Harper Lee|\"The unforgettabl...|also a classic.Co...|            dramatic|and deeply moving|\"\"To Kill A Mocki...|\n",
      "|  1|Harry Potter and ...| J.K. Rowling|Harry Potter thin...|['Fantasy', 'Fict...|                4.47|        9,278,135|https://www.goodr...|\n",
      "|  2| Pride and Prejudice|  Jane Austen|\"Since its immedi...|    Elizabeth Bennet|\"\"as delightful a...|        Mr. Darcy|is a splendid per...|\n",
      "|  3|The Diary of a Yo...|   Anne Frank|Discovered in the...|['Classics', 'Non...|                4.18|        3,488,438|https://www.goodr...|\n",
      "|  4|         Animal Farm|George Orwell|Librarian's note:...|['Classics', 'Fic...|                3.98|        3,575,172|https://www.goodr...|\n",
      "+---+--------------------+-------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+----------+--------------------+--------------------+----------+----------+----------+----------+-------------------+-----------------+-------+--------------------+--------------+-----+--------------------+---------------+------------------+--------+-----------+--------------------------+-----------+-----------+--------------+---------+---------+--------+-----------------+\n",
      "|   job_id|company_id|               title|         description|max_salary|med_salary|min_salary|pay_period|formatted_work_type|         location|applies|original_listed_time|remote_allowed|views|     job_posting_url|application_url|  application_type|  expiry|closed_time|formatted_experience_level|skills_desc|listed_time|posting_domain|sponsored|work_type|currency|compensation_type|\n",
      "+---------+----------+--------------------+--------------------+----------+----------+----------+----------+-------------------+-----------------+-------+--------------------+--------------+-----+--------------------+---------------+------------------+--------+-----------+--------------------------+-----------+-----------+--------------+---------+---------+--------+-----------------+\n",
      "| 85008768|      NULL|Licensed Insuranc...|While many indust...|     52000|      NULL|     45760|    YEARLY|          Full-time|        Chico, CA|   NULL|            1.69E+12|          NULL|    5|https://www.linke...|           NULL|ComplexOnsiteApply|1.71E+12|       NULL|                      NULL|       NULL|   1.69E+12|          NULL|        1|FULL_TIME|     USD|      BASE_SALARY|\n",
      "|133114754|  77766802|       Sales Manager|Are you a dynamic...|      NULL|      NULL|      NULL|      NULL|          Full-time|Santa Clarita, CA|   NULL|            1.69E+12|          NULL| NULL|https://www.linke...|           NULL|ComplexOnsiteApply|1.70E+12|       NULL|                      NULL|       NULL|   1.69E+12|          NULL|        0|FULL_TIME|    NULL|             NULL|\n",
      "|133196985|   1089558|  Model Risk Auditor|Join Us as a Mode...|      NULL|      NULL|      NULL|      NULL|           Contract|     New York, NY|      1|            1.69E+12|          NULL|   17|https://www.linke...|           NULL|ComplexOnsiteApply|1.70E+12|       NULL|                      NULL|       NULL|   1.69E+12|          NULL|        0| CONTRACT|    NULL|             NULL|\n",
      "|381055942|  96654609|    Business Manager|Business ManagerF...|      NULL|      NULL|      NULL|      NULL|          Full-time|       Forney, TX|   NULL|            1.69E+12|          NULL| NULL|https://www.linke...|           NULL|ComplexOnsiteApply|1.70E+12|       NULL|                      NULL|       NULL|   1.69E+12|          NULL|        0|FULL_TIME|    NULL|             NULL|\n",
      "|529257371|   1244539| NY Studio Assistant|YOU COULD BE ONE ...|      NULL|      NULL|      NULL|      NULL|          Full-time|     New York, NY|   NULL|            1.69E+12|          NULL|    2|https://www.linke...|           NULL|ComplexOnsiteApply|1.71E+12|       NULL|                      NULL|       NULL|   1.69E+12|          NULL|        1|FULL_TIME|    NULL|             NULL|\n",
      "+---------+----------+--------------------+--------------------+----------+----------+----------+----------+-------------------+-----------------+-------+--------------------+--------------+-----+--------------------+---------------+------------------+--------+-----------+--------------------------+-----------+-----------+--------------+---------+---------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|         description|   medical_specialty|         sample_name|       transcription|            keywords|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|A 23-year-old whi...|Allergy / Immunology|   Allergic Rhinitis|SUBJECTIVE:,  Thi...|allergy / immunol...|\n",
      "|  1|Consult for lapar...|          Bariatrics|Laparoscopic Gast...|PAST MEDICAL HIST...|bariatrics, lapar...|\n",
      "|  2|Consult for lapar...|          Bariatrics|Laparoscopic Gast...|\"HISTORY OF PRESE...|at his highest he...|\n",
      "|  3|2-D M-Mode. Doppler.|Cardiovascular / ...|2-D Echocardiogra...|2-D M-MODE: , ,1....|cardiovascular / ...|\n",
      "|  4|  2-D Echocardiogram|Cardiovascular / ...|2-D Echocardiogra...|1.  The left vent...|cardiovascular / ...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newsData = [category: string, filename: string ... 2 more fields]\n",
       "bookData = [_c0: string, Book: string ... 6 more fields]\n",
       "jobData = [job_id: string, company_id: string ... 25 more fields]\n",
       "medicalData = [_c0: int, description: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: int, description: string ... 4 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newsData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\",\"\\t\").option(\"multiLine\", \"true\")\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\").option(\"ignoreTrailingWhiteSpace\", \"true\").csv(root + \"datasets/\" + fileNames(0));\n",
    "newsData.show(5);\n",
    "//newsData.printSchema();\n",
    "\n",
    "//Reads the second file (book data)\n",
    "val bookData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\").option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\").csv(root + \"datasets/\" + fileNames(1));\n",
    "bookData.show(5);\n",
    "//bookData.printSchema();\n",
    "\n",
    "//Reads the third file (job postings)  (It does not seem to parse correctly but it is sufficient to show and test my function)\n",
    "val jobData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\").option(\"multiLine\", \"true\").csv(root + \"datasets/job_postings/\" + fileNames(2));\n",
    "jobData.show(5);\n",
    "//jobData.printSchema();\n",
    "\n",
    "//Reads the fourth file (medical transcripts)\n",
    "val medicalData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "                 .option(\"ignoreTrailingWhiteSpace\", \"true\").option(\"multiLine\", \"true\").csv(root + \"datasets/\" + fileNames(3));\n",
    "medicalData.show(5);\n",
    "//medicalData.printSchema();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39054fc",
   "metadata": {},
   "source": [
    "#### Extracting and pre-processing each of the dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3660b081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newsNL = [news article: string]\n",
       "bookNL = [book description: string]\n",
       "jobNL = [job posting: string]\n",
       "medicalNL = [medical transcript: string]\n",
       "dfNLArray = Array([news article: string], [book description: string], [job posting: string], [medical transcript: string])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array([news article: string], [book description: string], [job posting: string], [medical transcript: string])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creates dataframe with natural language under column \"news article\"\n",
    "val newsNL = newsData.select(newsData(\"content\").alias(\"news article\"));\n",
    "\n",
    "//Creates dataframe with natural language under column \"book description\"\n",
    "val bookNL = bookData.select(bookData(\"Description\").alias(\"book description\"));\n",
    "\n",
    "//Creates dataframe with natural language under column \"job posting\"\n",
    "val jobNL = jobData.select(jobData(\"description\").alias(\"job posting\"));\n",
    "\n",
    "//Creates dataframe with natural language under column \"medical transcript\"\n",
    "val medicalNL = medicalData.select(medicalData(\"transcription\").alias(\"medical transcript\"));\n",
    "\n",
    "val dfNLArray = Array(newsNL, bookNL, jobNL, medicalNL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d1232d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        news article|\n",
      "+--------------------+\n",
      "|quarterly profits...|\n",
      "|the dollar has hi...|\n",
      "|the owners of emb...|\n",
      "|british airways h...|\n",
      "|shares in uk drin...|\n",
      "|japan s economy t...|\n",
      "|the us created fe...|\n",
      "|india  which atte...|\n",
      "|ethiopia produced...|\n",
      "|a us government c...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of rows:2225\n",
      "+--------------------+\n",
      "|    book description|\n",
      "+--------------------+\n",
      "|harry potter thin...|\n",
      "|it s christmas ti...|\n",
      "|librarian s note ...|\n",
      "|orphaned as a chi...|\n",
      "|in romeo and juli...|\n",
      "| this beloved boo...|\n",
      "|you can find the ...|\n",
      "|   i can t explai...|\n",
      "|scarlett o hara  ...|\n",
      "|an award or prese...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of rows:2238\n",
      "+--------------------+\n",
      "|         job posting|\n",
      "+--------------------+\n",
      "|are you a dynamic...|\n",
      "|join our growing ...|\n",
      "|position  lead ed...|\n",
      "| service   ancill...|\n",
      "| you belong here ...|\n",
      "| you belong here ...|\n",
      "|who we are multi ...|\n",
      "|are you a fashion...|\n",
      "|meltzer lippe is ...|\n",
      "|welcome to sensib...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of rows:2218\n",
      "+--------------------+\n",
      "|  medical transcript|\n",
      "+--------------------+\n",
      "|subjective   this...|\n",
      "|past medical hist...|\n",
      "| history of prese...|\n",
      "|2 d m mode     1 ...|\n",
      "|2 d echocardiogra...|\n",
      "|preoperative diag...|\n",
      "|preoperative diag...|\n",
      "|2 d study 1  mild...|\n",
      "|preoperative diag...|\n",
      "|history of presen...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Number of rows:2202\n",
      "[Lorg.apache.spark.sql.Dataset;@72457db3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampleSize = 2225\n",
       "seed = 12345\n",
       "index = 4\n",
       "roughProcessedDF = Array([news article: string], [book description: string], [job posting: string], [medical transcript: string])\n",
       "regexPattern = \\-|_|\\s+|\\p{Punct}|^\\s+|\\s+$\n",
       "headers = Array(news article, book description, job posting, medical transcript)\n",
       "index = 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val sampleSize = 2225  // Number of rows you want in the sample\n",
    "val seed = 12345  // Seed for reproducibility\n",
    "\n",
    "var index = 0;\n",
    "val roughProcessedDF : Array[DataFrame] = new Array[DataFrame](4);\n",
    "val regexPattern = \"\"\"\\-|_|\\s+|\\p{Punct}|^\\s+|\\s+$\"\"\";\n",
    "val headers = Array(\"news article\", \"book description\", \"job posting\", \"medical transcript\");\n",
    "\n",
    "index=0;\n",
    "//Removes punctuations and whitespace\n",
    "for (df <- dfNLArray) {\n",
    "    val modifiedDF = df.withColumn(headers(index), lower(regexp_replace(col(headers(index)), regexPattern, \" \")));\n",
    "    val cutDF = modifiedDF.sample(false, sampleSize.toDouble / modifiedDF.count, seed);\n",
    "    cutDF.show(10);\n",
    "    println(\"Number of rows:\" + cutDF.count());\n",
    "    roughProcessedDF(index) = cutDF;\n",
    "    index+=1;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a09f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the documents is 2269 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "totalLength = 20156063\n",
       "totalCount = 8883\n",
       "averageLength = 2269\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2269"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Calculates the average length of the pre-processed natural language\n",
    "var totalLength : Long = 0;\n",
    "var totalCount : Long = 0;\n",
    "for (df <- roughProcessedDF) {\n",
    "    totalCount = totalCount + df.count();\n",
    "    var length : Long = 0;\n",
    "    for (row <- df.collect())\n",
    "        length = length + row.mkString.length;\n",
    "    totalLength = totalLength + length;\n",
    "}\n",
    "val averageLength = totalLength / totalCount;\n",
    "println(s\"The average length of the documents is $averageLength characters\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34dcb1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        news article|\n",
      "+--------------------+\n",
      "|quarterly profits...|\n",
      "|the dollar has hi...|\n",
      "|the owners of emb...|\n",
      "|british airways h...|\n",
      "|shares in uk drin...|\n",
      "|japan s economy t...|\n",
      "|the us created fe...|\n",
      "|india  which atte...|\n",
      "|ethiopia produced...|\n",
      "|a us government c...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+\n",
      "|    book description|\n",
      "+--------------------+\n",
      "|harry potter thin...|\n",
      "|it s christmas ti...|\n",
      "|librarian s note ...|\n",
      "|orphaned as a chi...|\n",
      "|in romeo and juli...|\n",
      "| this beloved boo...|\n",
      "|you can find the ...|\n",
      "|   i can t explai...|\n",
      "|scarlett o hara  ...|\n",
      "|an award or prese...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+\n",
      "|         job posting|\n",
      "+--------------------+\n",
      "|are you a dynamic...|\n",
      "|join our growing ...|\n",
      "|position  lead ed...|\n",
      "| service   ancill...|\n",
      "| you belong here ...|\n",
      "| you belong here ...|\n",
      "|who we are multi ...|\n",
      "|are you a fashion...|\n",
      "|meltzer lippe is ...|\n",
      "|welcome to sensib...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+\n",
      "|  medical transcript|\n",
      "+--------------------+\n",
      "|subjective   this...|\n",
      "|past medical hist...|\n",
      "| history of prese...|\n",
      "|2 d m mode     1 ...|\n",
      "|2 d echocardiogra...|\n",
      "|preoperative diag...|\n",
      "|preoperative diag...|\n",
      "|2 d study 1  mild...|\n",
      "|preoperative diag...|\n",
      "|history of presen...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "processedDF = Array([news article: string], [book description: string], [job posting: string], [medical transcript: string])\n",
       "index = 4\n",
       "maxLength = 2269\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2269"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val processedDF : Array[DataFrame] = new Array[DataFrame](4);\n",
    "index=0;\n",
    "val maxLength = averageLength;\n",
    "//Cuts the natural language which is longer than the average length\n",
    "for (df <- roughProcessedDF) {\n",
    "    val truncatedDF = df.withColumn(headers(index), expr(s\"substring(`${headers(index)}`, 1, $maxLength)\"));\n",
    "    truncatedDF.show(10);\n",
    "    processedDF(index) = truncatedDF;\n",
    "    index+=1;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e8c5068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        news article|\n",
      "+--------------------+\n",
      "|quarterly profits...|\n",
      "|the dollar has hi...|\n",
      "|the owners of emb...|\n",
      "|british airways h...|\n",
      "|shares in uk drin...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2225\n",
      "+--------------------+\n",
      "|    book description|\n",
      "+--------------------+\n",
      "|harry potter thin...|\n",
      "|it s christmas ti...|\n",
      "|librarian s note ...|\n",
      "|orphaned as a chi...|\n",
      "|in romeo and juli...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2200\n",
      "+--------------------+\n",
      "|         job posting|\n",
      "+--------------------+\n",
      "|are you a dynamic...|\n",
      "|join our growing ...|\n",
      "|position  lead ed...|\n",
      "|service   ancilla...|\n",
      "|you belong here  ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2209\n",
      "+--------------------+\n",
      "|  medical transcript|\n",
      "+--------------------+\n",
      "|subjective   this...|\n",
      "|past medical hist...|\n",
      "|history of presen...|\n",
      "|2 d m mode     1 ...|\n",
      "|2 d echocardiogra...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode.Overwrite\n",
    "//Saves the dataframes as processed csv class files\n",
    "processedDF(0).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedDatasets/\" + fileNames(0));\n",
    "processedDF(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedDatasets/\" + fileNames(1));\n",
    "processedDF(2).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedDatasets/\" + fileNames(2));\n",
    "processedDF(3).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedDatasets/\" + fileNames(3));\n",
    "\n",
    "//Checks that data saved correctly\n",
    "for (file <- fileNames) {\n",
    "    val temp = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"processedDatasets/\" + file);\n",
    "    temp.show(5);\n",
    "    println(temp.count());\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b0faa",
   "metadata": {},
   "source": [
    "## 2. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5658f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.util.Random\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"ID2221 Project\").master(\"local[*]\").getOrCreate();\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "/* \n",
    "The function takes a sequence of pre-processed class files and combines them into an evenly balanced data set which it then \n",
    "splits into smaller sets\n",
    ":param processedFiles: a string array which contains the paths to all the class files which should be considered by the function\n",
    ":param relativeDist: an array of doubles which determines how the created training sets should be distributed (and how many should be created)\n",
    ":param outputDirectory: a string which is the path to the output directory were the created sets will be stored\n",
    ":param outputNames: an array containing the names of the output files corresponding to the distribution given by relativeDist\n",
    ":param classSize: a integer specifying how many entries of each class the combined output sets should contain, by default as large as possible\n",
    "*/\n",
    "def createDataSets(processedFiles : Array[String], relativeDist : Array[Double], outputDirectory : String, \n",
    "                   outputNames : Array[String], classSize : Int = 0 ) {\n",
    "    //Creates an array holding dataframes of each input file\n",
    "    val fileDF: Array[DataFrame] = new Array[DataFrame](processedFiles.length);\n",
    "    for (i <- 0 to fileDF.length-1)\n",
    "        fileDF(i) = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(processedFiles(i));\n",
    "    \n",
    "    //Calculates desired size\n",
    "    var size : Int = classSize;\n",
    "    if (size <= 0) { //If user has not specified a desired size, the fuction will create a data set which is as large as possible\n",
    "        var minSize = fileDF(0).count();\n",
    "        for (i <- 1 to fileDF.length-1) {\n",
    "            val tempSize = fileDF(i).count();\n",
    "            if (tempSize < minSize) \n",
    "                minSize=tempSize;\n",
    "        }\n",
    "        size=minSize.asInstanceOf[Int];\n",
    "    }\n",
    "    \n",
    "    //Take \"size\" random rows from each dataframe\n",
    "    val sampledDF: Array[DataFrame] = new Array[DataFrame](fileDF.length);\n",
    "    for (i <- 0 to sampledDF.length-1) //Takes \"size\" random samples from each file\n",
    "        sampledDF(i) = fileDF(i).orderBy(rand()).limit(size);\n",
    "    \n",
    "    //Create dataframes with columns \"class\" and \"value\"\n",
    "    val pairDF: Array[DataFrame] = new Array[DataFrame](sampledDF.length);\n",
    "    for (i <- 0 to pairDF.length-1) {\n",
    "        pairDF(i)=sampledDF(i).withColumn(\"class\", lit(sampledDF(i).columns(0))); //Adds \"class\" column with value of original header\n",
    "        pairDF(i)=pairDF(i).withColumnRenamed(sampledDF(i).columns(0), \"value\"); //Renames original header to \"value\"\n",
    "    }\n",
    "    \n",
    "    //Splits the created pairs into parts according to \"relativeDist\"\n",
    "    val splitDF: Array[Array[DataFrame]] = new Array[Array[DataFrame]](sampledDF.length);\n",
    "    for (i <- 0 to splitDF.length-1) {\n",
    "        //The function sorts the results based on first column after picking random elements (we reshuffle when combining)\n",
    "        splitDF(i) = pairDF(i).randomSplit(relativeDist, 0); \n",
    "    }\n",
    "    \n",
    "    //Combines the split pairs into the desired sets (one set for each entry in \"relativeDist\")\n",
    "    val setDF: Array[DataFrame] = new Array[DataFrame](relativeDist.length);\n",
    "    for (i <- 0 to relativeDist.length-1) {\n",
    "        //Combines all the class dataframes\n",
    "        setDF(i) = splitDF(0)(i);\n",
    "        for (ii <- 1 to splitDF.length-1)\n",
    "            setDF(i) = setDF(i).union(splitDF(ii)(i));\n",
    "        \n",
    "        //Shuffles the result\n",
    "        setDF(i) = setDF(i).orderBy(rand());\n",
    "    }\n",
    "    \n",
    "    //Shows some samples from each created set\n",
    "    for (v <- setDF) {\n",
    "        v.show(10);\n",
    "        println(v.count());\n",
    "    }\n",
    "    \n",
    "    //Saves the created sets as csv files\n",
    "    for (i <- 0 to setDF.length-1)\n",
    "        setDF(i).write.mode(\"overwrite\").option(\"header\",\"true\").csv(outputDirectory + outputNames(i));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9fc84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rootProcessed : String = \"hdfs://localhost:9000/processedDatasets/\";\n",
    "val processedFiles : Array[String] = Array(rootProcessed + \"bbc-news-data.csv\", rootProcessed + \"goodreads_data.csv\", \n",
    "                                           rootProcessed + \"job_postings.csv\", rootProcessed + \"mtsamples.csv\");\n",
    "// In our project we used three set, 80% train, 15% validation and 5% test\n",
    "val dist : Array[Double] = Array(160, 30, 10); //Any values work, function only look at comparative sizes\n",
    "\n",
    "val outputNames : Array[String] = Array(\"train\", \"val\", \"test\");\n",
    "\n",
    "createDataSets(processedFiles, dist, \"hdfs://localhost:9000/project/dataSets/\", outputNames);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0721da4",
   "metadata": {},
   "source": [
    "## 3. Machine Learning on created data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{IDF,HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "//Loads the created data sets\n",
    "val train = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"dataSets/train\");\n",
    "val test = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"dataSets/test\");\n",
    "\n",
    "//Adding column which maps classes to integers to allow for spark ml library processing\n",
    "val mapper = spark.createDataFrame(Seq((\"book description\", 1.0),(\"news article\", 2.0),(\"medical transcript\", 3.0)\n",
    "                                       ,(\"job posting\", 4.0))).toDF(\"class\", \"label\");\n",
    "val trainSet = train.join(mapper, \"class\");\n",
    "val testSet = test.join(mapper, \"class\");\n",
    "\n",
    "//Number of words\n",
    "val totalSet = train.union(test);\n",
    "val nWords = totalSet.select(\"value\").flatMap(_.toString.split(\" \")).distinct().count();\n",
    "\n",
    "//Creating the pipeline for a simple classifier\n",
    "val tokenizer = new Tokenizer().setInputCol(\"value\").setOutputCol(\"words\");\n",
    "val hashTF = new HashingTF().setInputCol(tokenizer.getOutputCol).setOutputCol(\"hashTF\").setNumFeatures(nWords.asInstanceOf[Int]);\n",
    "val tf_idf = new IDF().setInputCol(hashTF.getOutputCol).setOutputCol(\"tf_idf\").setMinDocFreq(5);\n",
    "val model = new LogisticRegression().setMaxIter(20).setRegParam(0.01).setLabelCol(\"label\").setFeaturesCol(tf_idf.getOutputCol)\n",
    "    .setFamily(\"multinomial\");\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashTF, tf_idf, model));\n",
    "\n",
    "//Sets upp hyper parameter tuning using cross-validation\n",
    "val evaluator = new MulticlassClassificationEvaluator().setPredictionCol(\"prediction\")\n",
    "val paramGrid = new ParamGridBuilder().addGrid(model.regParam, Array(0.03, 0.01, 0.1)).addGrid(model.maxIter, Array(3,10,30)).build();\n",
    "val crossValidator = new CrossValidator().setEstimator(pipeline).setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid).setNumFolds(4).setParallelism(2);\n",
    "    \n",
    "//Train the model\n",
    "val trainedModel = pipeline.fit(trainSet);         //Set parameters (Fast with good performance)\n",
    "//val trainedModel = crossValidator.fit(trainSet); //Simple hyper parameter tuning using cross-validation (Slow with slightly better performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Evaluate performance of the model\n",
    "val accuracy = evaluator.evaluate(trainedModel.transform(testSet));\n",
    "println(s\"The trained model had an accuracy of $accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Save the trained model on disk\n",
    "trainedModel.write.overwrite().save(root + \"model/trainedLogisticRegressionModel\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988ab19",
   "metadata": {},
   "source": [
    "## JUST TRYING TO CONNECT HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index=0\n",
    "for (df <- processedDF) {\n",
    "    val hdfsOutputPath = \"hdfs://54.211.123.215:9000/processedFiles/\"+fileNames(index)\n",
    "// Assuming 'yourDataFrame' is your DataFrame with a column containing strings\n",
    "    df.write\n",
    "      .format(\"csv\")\n",
    "      .option(\"header\", \"true\") // If your CSV has a header\n",
    "      .save(hdfsOutputPath)\n",
    "    index+=1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793759cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import necessary libraries\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "// Define the JDBC connection properties\n",
    "val jdbcUrl = \"jdbc:mysql://your_mysql_host:your_mysql_port/your_database\"\n",
    "val connectionProperties = new java.util.Properties()\n",
    "connectionProperties.setProperty(\"user\", \"your_username\")\n",
    "connectionProperties.setProperty(\"password\", \"your_password\")\n",
    "\n",
    "// Define your DataFrame\n",
    "for (df <- processedDF) {\n",
    "    df.write.mode(SaveMode.Overwrite) // Choose the save mode you prefer\n",
    "      .jdbc(jdbcUrl, \"your_table_name\", connectionProperties)\n",
    "    \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
