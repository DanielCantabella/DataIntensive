{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0786e41c",
   "metadata": {},
   "source": [
    "#### Creating dataFrames for each of the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444bdf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root = hdfs://54.211.123.215:9000/\n",
       "spark = org.apache.spark.sql.SparkSession@4703e30b\n",
       "fileNames = Array(bbc-news-data.csv, goodreads_data.csv, job_postings.csv, mtsamples.csv)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(bbc-news-data.csv, goodreads_data.csv, job_postings.csv, mtsamples.csv)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions.{col, expr}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.util.matching \n",
    "import java.io.File\n",
    "\n",
    "\n",
    "//val root = new File(\".\").getCanonicalPath+\"/\"\n",
    "val root = \"hdfs://localhost:9000/\"\n",
    "val spark = SparkSession.builder().appName(\"ID2221 Project\").master(\"local[*]\").getOrCreate();\n",
    "val fileNames = Array(\"bbc-news-data.csv\",\"goodreads_data.csv\",\"job_postings.csv\",\"mtsamples.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd31b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newsSchema = StructType(StructField(category,StringType,false),StructField(filename,StringType,false),StructField(title,StringType,false),StructField(content,StringType,false))\n",
       "bookSchema = StructType(StructField(null,StringType,true),StructField(Book,StringType,true),StructField(Author,StringType,true),StructField(Description,StringType,true),StructField(Genres,StringType,true),StructField(Avg_Rating,DoubleType,true),StructField(Num_Ratings,IntegerType,true),StructField(URL,StringType,true))\n",
       "jobSchema = StructType(StructField(job_id,IntegerType,true),StructField(company_id,IntegerType,true),StructField(title,StringType,true),StructField(description,StringType,true),Struc...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(job_id,IntegerType,true),StructField(company_id,IntegerType,true),StructField(title,StringType,true),StructField(description,StringType,true),Struc..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Schemas (not necessary but just to show what we are dealing with)\n",
    "val newsSchema = StructType(Array(\n",
    "  StructField(\"category\", StringType, nullable = false),\n",
    "  StructField(\"filename\", StringType, nullable = false),\n",
    "  StructField(\"title\", StringType, nullable = false),\n",
    "  StructField(\"content\", StringType, nullable = false)\n",
    "));\n",
    "val bookSchema = StructType(Array(\n",
    "  StructField(\"null\", StringType, nullable = true),\n",
    "  StructField(\"Book\", StringType, nullable = true),\n",
    "  StructField(\"Author\", StringType, nullable = true),\n",
    "  StructField(\"Description\", StringType, nullable = true),\n",
    "  StructField(\"Genres\", StringType, nullable = true),\n",
    "  StructField(\"Avg_Rating\", DoubleType, nullable = true),\n",
    "  StructField(\"Num_Ratings\", IntegerType, nullable = true),\n",
    "  StructField(\"URL\", StringType, nullable = true),\n",
    "));\n",
    "val jobSchema = StructType(Array(\n",
    "  StructField(\"job_id\", IntegerType, nullable = true),\n",
    "  StructField(\"company_id\", IntegerType, nullable = true),\n",
    "  StructField(\"title\", StringType, nullable = true),\n",
    "  StructField(\"description\", StringType, nullable = true),\n",
    "  StructField(\"max_salary\", IntegerType, nullable = true),\n",
    "  StructField(\"med_salary\", IntegerType, nullable = true),\n",
    "  StructField(\"min_salary\", IntegerType, nullable = true),\n",
    "  StructField(\"pay_period\", StringType, nullable = true),\n",
    "  StructField(\"formatted_work_type\", StringType, nullable = true),\n",
    "  StructField(\"location\", StringType, nullable = true),\n",
    "  StructField(\"applies\", IntegerType, nullable = true),\n",
    "  StructField(\"original_listed_time\", DoubleType, nullable = true),\n",
    "  StructField(\"remote_allowed\", IntegerType, nullable = true),\n",
    "  StructField(\"views\", IntegerType, nullable = true),\n",
    "  StructField(\"job_posting_url\", StringType, nullable = true),\n",
    "  StructField(\"application_url\", StringType, nullable = true),\n",
    "  StructField(\"application_type\", StringType, nullable = true),\n",
    "  StructField(\"expiry\", DoubleType, nullable = true),\n",
    "  StructField(\"closed_time\", DoubleType, nullable = true),\n",
    "  StructField(\"formatted_experience_level\", StringType, nullable = true),\n",
    "  StructField(\"skills_desc\", StringType, nullable = true),\n",
    "  StructField(\"listed_time\", DoubleType, nullable = true),\n",
    "  StructField(\"posting_domain\", StringType, nullable = true),\n",
    "  StructField(\"sponsored\", IntegerType, nullable = true),\n",
    "  StructField(\"work_type\", StringType, nullable = true),\n",
    "  StructField(\"currency\", StringType, nullable = true),\n",
    "  StructField(\"compensation_type\", StringType, nullable = true)\n",
    "));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val newsData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\",\"\\t\").option(\"multiLine\", \"true\")\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\").option(\"ignoreTrailingWhiteSpace\", \"true\").csv(root + \"rawFiles/\" + fileNames(0));\n",
    "newsData.show(5);\n",
    "//newsData.printSchema();\n",
    "\n",
    "//Reads the second file (book data)\n",
    "val bookData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\").option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\").csv(root + \"rawFiles/\" + fileNames(1));\n",
    "bookData.show(5);\n",
    "//bookData.printSchema();\n",
    "\n",
    "//Reads the third file (job postings)  (It does not seem to parse correctly but it is sufficient to show and test my function)\n",
    "val jobData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\").option(\"multiLine\", \"true\").csv(root + \"rawFiles/job_postings/\" + fileNames(2));\n",
    "jobData.show(5);\n",
    "//jobData.printSchema();\n",
    "\n",
    "//Reads the fourth file (medical transcripts)\n",
    "val medicalData = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "                 .option(\"ignoreTrailingWhiteSpace\", \"true\").option(\"multiLine\", \"true\").csv(root + \"rawFiles/\" + fileNames(3));\n",
    "medicalData.show(5);\n",
    "//medicalData.printSchema();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39054fc",
   "metadata": {},
   "source": [
    "#### Extracting and pre-processing each of the dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Creates dataframe with natural language under column \"news article\"\n",
    "val newsNL = newsData.select(newsData(\"content\").alias(\"news article\"));\n",
    "\n",
    "//Creates dataframe with natural language under column \"book description\"\n",
    "val bookNL = bookData.select(bookData(\"Description\").alias(\"book description\"));\n",
    "\n",
    "//Creates dataframe with natural language under column \"job posting\"\n",
    "val jobNL = jobData.select(jobData(\"description\").alias(\"job posting\"));\n",
    "\n",
    "//Creates dataframe with natural language under column \"medical transcript\"\n",
    "val medicalNL = medicalData.select(medicalData(\"transcription\").alias(\"medical transcript\"));\n",
    "\n",
    "val dfNLArray = Array(newsNL, bookNL, jobNL, medicalNL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1232d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val sampleSize = 2225  // Number of rows you want in the sample\n",
    "val seed = 12345  // Seed for reproducibility\n",
    "\n",
    "var index = 0;\n",
    "val roughProcessedDF : Array[DataFrame] = new Array[DataFrame](4);\n",
    "val regexPattern = \"\"\"\\-|_|\\s+|\\p{Punct}|^\\s+|\\s+$\"\"\";\n",
    "val headers = Array(\"news article\", \"book description\", \"job posting\", \"medical transcript\");\n",
    "\n",
    "index=0;\n",
    "//Removes punctuations and whitespace\n",
    "for (df <- dfNLArray) {\n",
    "    val modifiedDF = df.withColumn(headers(index), lower(regexp_replace(col(headers(index)), regexPattern, \" \")));\n",
    "    val cutDF = modifiedDF.sample(false, sampleSize.toDouble / modifiedDF.count, seed);\n",
    "    cutDF.show(10);\n",
    "    println(\"Number of rows:\" + cutDF.count());\n",
    "    roughProcessedDF(index) = cutDF;\n",
    "    index+=1;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Calculates the average length of the pre-processed natural language\n",
    "var totalLength : Long = 0;\n",
    "var totalCount : Long = 0;\n",
    "for (df <- roughProcessedDF) {\n",
    "    totalCount = totalCount + df.count();\n",
    "    var length : Long = 0;\n",
    "    for (row <- df.collect())\n",
    "        length = length + row.mkString.length;\n",
    "    totalLength = totalLength + length;\n",
    "}\n",
    "val averageLength = totalLength / totalCount;\n",
    "println(s\"The average length of the documents is $averageLength characters\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcb1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val processedDF : Array[DataFrame] = new Array[DataFrame](4);\n",
    "index=0;\n",
    "val maxLength = averageLength;\n",
    "//Cuts the natural language which is longer than the average length\n",
    "for (df <- roughProcessedDF) {\n",
    "    val truncatedDF = df.withColumn(headers(index), expr(s\"substring(`${headers(index)}`, 1, $maxLength)\"));\n",
    "    truncatedDF.show(10);\n",
    "    processedDF(index) = truncatedDF;\n",
    "    index+=1;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SaveMode.Overwrite\n",
    "//Saves the dataframes as processed csv class files\n",
    "processedDF(0).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedDatasets/\" + fileNames(0));\n",
    "processedDF(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedDatasets/\" + fileNames(1));\n",
    "processedDF(2).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedDatasets/\" + fileNames(2));\n",
    "processedDF(3).write.mode(\"overwrite\").option(\"header\",\"true\").csv(root + \"processedDatasets/\" + fileNames(3));\n",
    "\n",
    "//Checks that data saved correctly\n",
    "for (file <- fileNames) {\n",
    "    val temp = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"processedDatasets/\" + file);\n",
    "    temp.show(5);\n",
    "    println(temp.count());\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b0faa",
   "metadata": {},
   "source": [
    "## 2. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5658f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import scala.util.Random\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"ID2221 Project\").master(\"local[*]\").getOrCreate();\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "/* \n",
    "The function takes a sequence of pre-processed class files and combines them into an evenly balanced data set which it then \n",
    "splits into smaller sets\n",
    ":param processedFiles: a string array which contains the paths to all the class files which should be considered by the function\n",
    ":param relativeDist: an array of doubles which determines how the created training sets should be distributed (and how many should be created)\n",
    ":param outputDirectory: a string which is the path to the output directory were the created sets will be stored\n",
    ":param outputNames: an array containing the names of the output files corresponding to the distribution given by relativeDist\n",
    ":param classSize: a integer specifying how many entries of each class the combined output sets should contain, by default as large as possible\n",
    "*/\n",
    "def createDataSets(processedFiles : Array[String], relativeDist : Array[Double], outputDirectory : String, \n",
    "                   outputNames : Array[String], classSize : Int = 0 ) {\n",
    "    //Creates an array holding dataframes of each input file\n",
    "    val fileDF: Array[DataFrame] = new Array[DataFrame](processedFiles.length);\n",
    "    for (i <- 0 to fileDF.length-1)\n",
    "        fileDF(i) = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(processedFiles(i));\n",
    "    \n",
    "    //Calculates desired size\n",
    "    var size : Int = classSize;\n",
    "    if (size <= 0) { //If user has not specified a desired size, the fuction will create a data set which is as large as possible\n",
    "        var minSize = fileDF(0).count();\n",
    "        for (i <- 1 to fileDF.length-1) {\n",
    "            val tempSize = fileDF(i).count();\n",
    "            if (tempSize < minSize) \n",
    "                minSize=tempSize;\n",
    "        }\n",
    "        size=minSize.asInstanceOf[Int];\n",
    "    }\n",
    "    \n",
    "    //Take \"size\" random rows from each dataframe\n",
    "    val sampledDF: Array[DataFrame] = new Array[DataFrame](fileDF.length);\n",
    "    for (i <- 0 to sampledDF.length-1) //Takes \"size\" random samples from each file\n",
    "        sampledDF(i) = fileDF(i).orderBy(rand()).limit(size);\n",
    "    \n",
    "    //Create dataframes with columns \"class\" and \"value\"\n",
    "    val pairDF: Array[DataFrame] = new Array[DataFrame](sampledDF.length);\n",
    "    for (i <- 0 to pairDF.length-1) {\n",
    "        pairDF(i)=sampledDF(i).withColumn(\"class\", lit(sampledDF(i).columns(0))); //Adds \"class\" column with value of original header\n",
    "        pairDF(i)=pairDF(i).withColumnRenamed(sampledDF(i).columns(0), \"value\"); //Renames original header to \"value\"\n",
    "    }\n",
    "    \n",
    "    //Splits the created pairs into parts according to \"relativeDist\"\n",
    "    val splitDF: Array[Array[DataFrame]] = new Array[Array[DataFrame]](sampledDF.length);\n",
    "    for (i <- 0 to splitDF.length-1) {\n",
    "        //The function sorts the results based on first column after picking random elements (we reshuffle when combining)\n",
    "        splitDF(i) = pairDF(i).randomSplit(relativeDist, 0); \n",
    "    }\n",
    "    \n",
    "    //Combines the split pairs into the desired sets (one set for each entry in \"relativeDist\")\n",
    "    val setDF: Array[DataFrame] = new Array[DataFrame](relativeDist.length);\n",
    "    for (i <- 0 to relativeDist.length-1) {\n",
    "        //Combines all the class dataframes\n",
    "        setDF(i) = splitDF(0)(i);\n",
    "        for (ii <- 1 to splitDF.length-1)\n",
    "            setDF(i) = setDF(i).union(splitDF(ii)(i));\n",
    "        \n",
    "        //Shuffles the result\n",
    "        setDF(i) = setDF(i).orderBy(rand());\n",
    "    }\n",
    "    \n",
    "    //Shows some samples from each created set\n",
    "    for (v <- setDF) {\n",
    "        v.show(10);\n",
    "        println(v.count());\n",
    "    }\n",
    "    \n",
    "    //Saves the created sets as csv files\n",
    "    for (i <- 0 to setDF.length-1)\n",
    "        setDF(i).write.mode(\"overwrite\").option(\"header\",\"true\").csv(outputDirectory + outputNames(i));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9fc84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val rootProcessed : String = \"hdfs://localhost:9000/processedDatasets/\";\n",
    "val processedFiles : Array[String] = Array(rootProcessed + \"bbc-news-data.csv\", rootProcessed + \"goodreads_data.csv\", \n",
    "                                           rootProcessed + \"job_postings.csv\", rootProcessed + \"mtsamples.csv\");\n",
    "// In our project we used three set, 80% train, 15% validation and 5% test\n",
    "val dist : Array[Double] = Array(160, 30, 10); //Any values work, function only look at comparative sizes\n",
    "\n",
    "val outputNames : Array[String] = Array(\"train\", \"val\", \"test\");\n",
    "\n",
    "createDataSets(processedFiles, dist, \"hdfs://localhost:9000/dataSets/\", outputNames);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0721da4",
   "metadata": {},
   "source": [
    "## 3. Machine Learning on created data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{IDF,HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "//Loads the created data sets\n",
    "val train = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"dataSetsML/train\");\n",
    "val test = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(root + \"dataSetsML/test\");\n",
    "\n",
    "//Adding column which maps classes to integers to allow for spark ml library processing\n",
    "val mapper = spark.createDataFrame(Seq((\"book description\", 1.0),(\"news article\", 2.0),(\"medical transcript\", 3.0)\n",
    "                                       ,(\"job posting\", 4.0))).toDF(\"class\", \"label\");\n",
    "val trainSet = train.join(mapper, \"class\");\n",
    "val testSet = test.join(mapper, \"class\");\n",
    "\n",
    "//Number of words\n",
    "val totalSet = train.union(test);\n",
    "val nWords = totalSet.select(\"value\").flatMap(_.toString.split(\" \")).distinct().count();\n",
    "\n",
    "//Creating the pipeline for a simple classifier\n",
    "val tokenizer = new Tokenizer().setInputCol(\"value\").setOutputCol(\"words\");\n",
    "val hashTF = new HashingTF().setInputCol(tokenizer.getOutputCol).setOutputCol(\"hashTF\").setNumFeatures(nWords.asInstanceOf[Int]);\n",
    "val tf_idf = new IDF().setInputCol(hashTF.getOutputCol).setOutputCol(\"tf_idf\").setMinDocFreq(5);\n",
    "val model = new LogisticRegression().setMaxIter(20).setRegParam(0.01).setLabelCol(\"label\").setFeaturesCol(tf_idf.getOutputCol)\n",
    "    .setFamily(\"multinomial\");\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashTF, tf_idf, model));\n",
    "\n",
    "//Sets upp hyper parameter tuning using cross-validation\n",
    "val evaluator = new MulticlassClassificationEvaluator().setPredictionCol(\"prediction\")\n",
    "val paramGrid = new ParamGridBuilder().addGrid(model.regParam, Array(0.03, 0.01, 0.1)).addGrid(model.maxIter, Array(3,10,30)).build();\n",
    "val crossValidator = new CrossValidator().setEstimator(pipeline).setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid).setNumFolds(4).setParallelism(2);\n",
    "    \n",
    "//Train the model\n",
    "val trainedModel = pipeline.fit(trainSet);         //Set parameters (Fast with good performance)\n",
    "//val trainedModel = crossValidator.fit(trainSet); //Simple hyper parameter tuning using cross-validation (Slow with slightly better performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Evaluate performance of the model\n",
    "val accuracy = evaluator.evaluate(trainedModel.transform(testSet));\n",
    "println(s\"The trained model had an accuracy of $accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Save the trained model on disk\n",
    "trainedModel.write.overwrite().save(root + \"model/trainedLogisticRegressionModel\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
